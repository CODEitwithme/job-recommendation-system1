{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u3L1F7JkR7N7"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import pandas as pd\n",
        "pd.options.mode.copy_on_write = True\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qb1aQ5Z4SVaf"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qX_IiFaES42E"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('naukri_com-job_sample.csv')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zlhin8AwTT7K"
      },
      "outputs": [],
      "source": [
        "def load_and_clean_data(filepath):\n",
        "    df = pd.read_csv(filepath)\n",
        "    df = df[['jobtitle', 'jobdescription', 'skills', 'education']]\n",
        "    df.dropna(inplace=True)\n",
        "    df.reset_index(drop=True, inplace=True)\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "opr3NNv_IC9x"
      },
      "outputs": [],
      "source": [
        "load_and_clean_data(\"naukri_com-job_sample.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lFIYqmqzS1IA"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import nltk\n",
        "import re\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# üîÉ Download required NLTK resources (run once)\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HRe-DJ8pX3-x"
      },
      "outputs": [],
      "source": [
        "df[['jobtitle', 'jobdescription', 'skills', 'education']] = df[['jobtitle', 'jobdescription', 'skills', 'education']].fillna('')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IUL38AoWYa2E"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "\n",
        "# Download to a custom directory\n",
        "nltk.download('punkt', download_dir='/content/nltk_data')\n",
        "nltk.data.path.append('/content/nltk_data')  # Add path to NLTK search list\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QiNrx4XCYgi2"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('punkt')                    # Sentence + word tokenizer\n",
        "nltk.download('stopwords')               # Common stopwords\n",
        "nltk.download('wordnet')                 # Lemmatizer base\n",
        "nltk.download('averaged_perceptron_tagger')  # For POS tagging if needed\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mR45OgZxbbTU"
      },
      "outputs": [],
      "source": [
        "# prompt: I want to do NLP operation do Preprocess and Vectorize\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import re\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "\n",
        "# Initialize stop words and lemmatizer\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove special characters and digits\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    # Tokenize\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    # Remove stop words and lemmatize\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
        "    # Join tokens back into a string\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Apply preprocessing to relevant columns\n",
        "df['processed_jobtitle'] = df['jobtitle'].apply(preprocess_text)\n",
        "df['processed_jobdescription'] = df['jobdescription'].apply(preprocess_text)\n",
        "df['processed_skills'] = df['skills'].apply(preprocess_text)\n",
        "df['processed_education'] = df['education'].apply(preprocess_text)\n",
        "\n",
        "# Combine processed text for vectorization\n",
        "df['combined_text'] = df['processed_jobtitle'] + ' ' + df['processed_jobdescription'] + ' ' + df['processed_skills'] + ' ' + df['processed_education']\n",
        "\n",
        "# Initialize TF-IDF Vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=5000) # You can adjust max_features\n",
        "\n",
        "# Fit and transform the combined text\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(df['combined_text'])\n",
        "\n",
        "print(\"Original DataFrame head:\")\n",
        "print(df.head())\n",
        "print(\"\\nProcessed DataFrame head (with new columns):\")\n",
        "print(df[['processed_jobtitle', 'processed_jobdescription', 'processed_skills', 'processed_education', 'combined_text']].head())\n",
        "print(\"\\nShape of TF-IDF matrix:\")\n",
        "tfidf_matrix.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dV20BTxTeKHF"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s91DQW3v6cKu"
      },
      "outputs": [],
      "source": [
        "# Load NLTK resources only once per environment\n",
        "import nltk\n",
        "\n",
        "resources = {\n",
        "    'stopwords': 'corpora/stopwords',\n",
        "    'punkt': 'tokenizers/punkt',\n",
        "    'wordnet': 'corpora/wordnet',\n",
        "    'averaged_perceptron_tagger': 'taggers/averaged_perceptron_tagger',\n",
        "    'omw-1.4': 'corpora/omw-1.4'\n",
        "}\n",
        "\n",
        "for name, path in resources.items():\n",
        "    try:\n",
        "        nltk.data.find(path)\n",
        "    except LookupError:\n",
        "        nltk.download(name)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4401e48f"
      },
      "outputs": [],
      "source": [
        "# Save the processed DataFrame\n",
        "df.to_csv('processed_job_data.csv', index=False)\n",
        "\n",
        "# Save the fitted TF-IDF vectorizer\n",
        "with open('tfidf_vectorizer.pkl', 'wb') as f:\n",
        "    pickle.dump(tfidf_vectorizer, f)\n",
        "\n",
        "print(\"\\nProcessed data and vectorizer saved.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3fTs7bOy7OBt"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# üîÉ Download required NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# üì¶ Load processed job data\n",
        "df_loaded = pd.read_csv('processed_job_data.csv')\n",
        "\n",
        "# üß† Load pre-trained BERT model (SBERT variant)\n",
        "bert_model = SentenceTransformer('all-MiniLM-L6-v2')  # Lightweight and fast\n",
        "\n",
        "# üßπ Initialize preprocessing tools\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def preprocess_text(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# üîÑ Precompute BERT embeddings for job descriptions\n",
        "df_loaded['cleaned_text'] = df_loaded['combined_text'].apply(preprocess_text)\n",
        "job_embeddings = bert_model.encode(df_loaded['cleaned_text'].tolist(), convert_to_tensor=True)\n",
        "\n",
        "def recommend_jobs(user_profile_input):\n",
        "    processed_user_input = preprocess_text(user_profile_input)\n",
        "    user_embedding = bert_model.encode([processed_user_input], convert_to_tensor=True)\n",
        "\n",
        "    # üéØ Calculate cosine similarity with all job embeddings\n",
        "    similarity_scores = cosine_similarity(user_embedding.cpu(), job_embeddings.cpu())[0]\n",
        "    top_job_indices = np.argsort(similarity_scores)[::-1][:3]\n",
        "\n",
        "    # üì¢ Display recommendations\n",
        "    print(\"\\nüîç Top 3 Job Recommendations:\")\n",
        "    for i, idx in enumerate(top_job_indices):\n",
        "        similarity_percent = round(similarity_scores[idx] * 100, 2)\n",
        "        print(f\"\\nüîπ Recommendation {i+1} (Match: {similarity_percent}%):\")\n",
        "        print(\"Job Title:\", df_loaded.loc[idx, 'jobtitle'])\n",
        "        print(\"Job Description:\", df_loaded.loc[idx, 'jobdescription'][:400] + '...')\n",
        "        print(\"Skills:\", df_loaded.loc[idx, 'skills'])\n",
        "        print(\"Education:\", df_loaded.loc[idx, 'education'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C9dyOAgVxx-k"
      },
      "outputs": [],
      "source": [
        "recommend_jobs(\"I am a data analyst skilled in Python, SQL, and machine learning\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# üíæ Save job embeddings and DataFrame\n",
        "with open('job_embeddings.pkl', 'wb') as f:\n",
        "    pickle.dump(job_embeddings, f)\n",
        "\n",
        "df_loaded.to_pickle('job_data_with_cleaned_text.pkl')\n"
      ],
      "metadata": {
        "id": "je95jrpaZqnA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import FileLink\n",
        "\n",
        "# Download links (works in Kaggle and Colab)\n",
        "FileLink('job_embeddings.pkl')  # Click to download\n",
        "FileLink('job_data_with_cleaned_text.pkl')  # Click to download\n"
      ],
      "metadata": {
        "id": "H6a3acbRhD_Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6LLm_xe-F4xM"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "# Save job embeddings\n",
        "with open('job_embeddings.pkl', 'wb') as f:\n",
        "    pickle.dump(job_embeddings, f)\n",
        "\n",
        "# Save the dataframe\n",
        "df_loaded.to_csv('processed_job_data.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YWT4HgxT9lvN"
      },
      "outputs": [],
      "source": [
        "# prompt: how to save and import this model so I can use this in my web directly\n",
        "\n",
        "# Save the processed DataFrame\n",
        "df.to_csv('processed_job_data.csv', index=False)\n",
        "\n",
        "# Save the fitted TF-IDF vectorizer\n",
        "with open('tfidf_vectorizer.pkl', 'wb') as f:\n",
        "    pickle.dump(tfidf_vectorizer, f)\n",
        "\n",
        "print(\"\\nProcessed data and vectorizer saved.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3cd99a92"
      },
      "source": [
        "import pickle\n",
        "\n",
        "# Save the BERT model\n",
        "with open('bert_model_new.pkl', 'wb') as f:\n",
        "    pickle.dump(bert_model, f)\n",
        "\n",
        "print(\"BERT model saved successfully as 'bert_new_model.pkl'\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.download('bert_model_new.pkl')"
      ],
      "metadata": {
        "id": "2e1kTSNphcxk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VbDTcH6ROYaw"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}